name: CI
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Install dependencies
        env:
          AIRFLOW_VERSION: 2.9.0
          PYTHON_VERSION: "3.11"
        run: |
          python -m pip install --upgrade pip
          # Install Airflow with pinned constraints (required for reliability)
          pip install "apache-airflow==${AIRFLOW_VERSION}" \
            --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
          # Project deps
          pip install -r requirements.txt
          # Dev/test tools
          pip install flake8 black pytest pyarrow pandas

      - name: Lint (flake8)
        run: flake8 airflow_job spark_job --max-line-length=120 --ignore=F401,E302,E305,E501

      - name: Format check (black)
        run: black --check --line-length 120 airflow_job spark_job

      - name: DAG import check
        run: |
          python - <<'PY'
          import importlib, os, sys
          sys.path.append('airflow_job')
          for root, _, files in os.walk('airflow_job'):
              for f in files:
                  if f.endswith('.py'):
                      mod = f[:-3]
                      print('Importing', mod)
                      importlib.import_module(mod)
          print('DAG import OK')
          PY

      - name: Spark smoke test
        run: |
          python - <<'PY'
          import os, tempfile, pandas as pd
          from pyspark.sql import SparkSession
          df = pd.DataFrame([{
            "num_passengers":2,"sales_channel":"Internet","trip_type":"RoundTrip","purchase_lead":10,
            "length_of_stay":5,"flight_hour":7,"flight_day":"Sat","route":"AKLDEL","booking_origin":"New Zealand",
            "wants_extra_baggage":1,"wants_preferred_seat":0,"wants_in_flight_meals":0,"flight_duration":5.5,"booking_complete":1
          }])
          with tempfile.TemporaryDirectory() as tmp:
            src = os.path.join(tmp, 'sample.csv'); out = os.path.join(tmp, 'out')
            df.to_csv(src, index=False)
            spark = SparkSession.builder.appName('ci-smoke').master('local[*]').getOrCreate()
            spark.read.csv(src, header=True, inferSchema=True).select('route').coalesce(1).write.mode('overwrite').csv(out, header=True)
            spark.stop()
            assert os.path.exists(out), 'Spark did not write output'
          print('Spark smoke test passed')
          PY
  docker-publish:
    name: Build & Push Airflow+Spark image
    runs-on: ubuntu-latest
    needs: ci
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/')
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - uses: docker/setup-qemu-action@v3
      - uses: docker/setup-buildx-action@v3

      - name: Docker meta (tags + labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=raw,value=latest,enable={{is_default_branch}}
            type=ref,event=tag
            type=ref,event=branch
            type=sha,format=short

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build & Push
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
