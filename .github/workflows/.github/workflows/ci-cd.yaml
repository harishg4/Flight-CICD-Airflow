name: CI
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  ci:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install flake8 black pytest pyarrow

      - name: Lint (flake8)
        run: flake8 airflow_job spark_job

      - name: Format check (black)
        run: black --check airflow_job spark_job

      - name: DAG import check
        run: |
          python - <<'PY'
          import importlib, os, sys
          sys.path.append('airflow_job')
          for root, _, files in os.walk('airflow_job'):
              for f in files:
                  if f.endswith('.py'):
                      mod = f[:-3]
                      print('Importing', mod)
                      importlib.import_module(mod)
          print('DAG import OK')
          PY

      - name: Spark smoke test
        run: |
          python - <<'PY'
          import os, tempfile, pandas as pd
          from pyspark.sql import SparkSession

          df = pd.DataFrame([
              {"num_passengers":2,"sales_channel":"Internet","trip_type":"RoundTrip","purchase_lead":10,
               "length_of_stay":5,"flight_hour":7,"flight_day":"Sat","route":"AKLDEL","booking_origin":"New Zealand",
               "wants_extra_baggage":1,"wants_preferred_seat":0,"wants_in_flight_meals":0,"flight_duration":5.5,"booking_complete":1},
          ])

          with tempfile.TemporaryDirectory() as tmp:
              src = os.path.join(tmp, 'sample.csv')
              out = os.path.join(tmp, 'out')
              df.to_csv(src, index=False)

              spark = (SparkSession.builder.appName('ci-smoke')
                                  .master('local[*]')
                                  .getOrCreate())
              sdf = spark.read.csv(src, header=True, inferSchema=True)
              sdf = sdf.select('route')
              sdf.coalesce(1).write.mode('overwrite').csv(out, header=True)
              spark.stop()

              assert os.path.exists(out), 'Spark did not write output'
          print('Spark smoke test passed')
          PY